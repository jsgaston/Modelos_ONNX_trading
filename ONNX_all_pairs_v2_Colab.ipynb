# ===================================================================
# MONTAR GOOGLE DRIVE PRIMERO
# ===================================================================
print("Montando Google Drive...")
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
print("‚úì Google Drive montado correctamente\n")

# ===================================================================
# INSTALACI√ìN DE LIBRER√çAS - Compatibles con Google Colab
# ===================================================================
print("Instalando librer√≠as con versiones compatibles...")

!pip install -q tf2onnx onnx scikit-learn pandas

print("‚úì Librer√≠as instaladas correctamente\n")

# ===================================================================
# PARCHE PARA tf2onnx - Arreglar compatibilidad con NumPy 2.0
# ===================================================================
print("Aplicando parche para tf2onnx...")

import os
import sys
import re

# Encontrar y parchear el archivo tfonnx.py de tf2onnx
tf2onnx_path = None
for path in sys.path:
    potential_path = os.path.join(path, 'tf2onnx', 'tfonnx.py')
    if os.path.exists(potential_path):
        tf2onnx_path = potential_path
        break

if tf2onnx_path:
    print(f"  Parcheando: {tf2onnx_path}")
    
    # Leer el archivo
    with open(tf2onnx_path, 'r') as f:
        content = f.read()
    
    # Reemplazar np.cast con una versi√≥n compatible
    if '"Cast": np.cast,' in content:
        # Crear backup
        backup_path = tf2onnx_path + '.backup'
        if not os.path.exists(backup_path):
            with open(backup_path, 'w') as f:
                f.write(content)
        
        # Aplicar parche
        content = content.replace(
            '"Cast": np.cast,',
            '"Cast": lambda x, dtype: np.asarray(x, dtype=dtype),'
        )
        
        # Guardar archivo parcheado
        with open(tf2onnx_path, 'w') as f:
            f.write(content)
        
        print("‚úì Parche aplicado exitosamente\n")
    else:
        print("‚úì El archivo ya est√° parcheado o no necesita parche\n")
else:
    print("‚ö†Ô∏è No se encontr√≥ tf2onnx para parchear\n")

# ===================================================================
# IMPORTAR LIBRER√çAS
# ===================================================================
import tensorflow as tf
import numpy as np
import pandas as pd
import os 
import shutil
import subprocess
from datetime import timedelta, datetime
from sklearn.preprocessing import MinMaxScaler

# Importar Keras desde TensorFlow
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Dropout, LSTM
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping

print("‚úì Librer√≠as importadas correctamente")

# ===================================================================
# CONFIGURAR RUTAS - AJUSTA ESTAS RUTAS SEG√öN TU ESTRUCTURA
# ===================================================================
DATA_PATH = '/content/drive/MyDrive/forex_data/'  # Ruta donde est√°n los CSVs
MODEL_PATH = '/content/drive/MyDrive/forex_models/'  # Ruta donde se guardar√°n los ONNX

# Crear carpeta de modelos si no existe
os.makedirs(MODEL_PATH, exist_ok=True)

print(f"üìÇ Data path para CSVs: {DATA_PATH}")
print(f"üíæ Model path para guardar ONNX: {MODEL_PATH}")

# ===================================================================
# PAR√ÅMETROS Y CONFIGURACI√ìN
# ===================================================================
symbols = ["USDCAD", "EURUSD", "GBPUSD", "USDCHF", "AUDUSD"]
inp_history_size = 120

# ===================================================================
# FUNCIONES
# ===================================================================

def split_sequence(sequence, n_steps):
    """Split a univariate sequence into samples"""
    X, y = list(), list()
    for i in range(len(sequence)):
       end_ix = i + n_steps
       if end_ix > len(sequence)-1:
          break
       seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
       X.append(seq_x)
       y.append(seq_y)
    return np.array(X), np.array(y)

def train_and_save_model(simbol, end_date, model_suffix=""):
    """
    Trains a model for a given symbol and time period.
    
    Parameters:
    - simbol: Trading symbol (e.g., "EURUSD")
    - end_date: End date for the training data
    - model_suffix: Suffix to add to model name (e.g., "backtesting")
    """
    start_date = end_date - timedelta(days=inp_history_size)
    
    if model_suffix:
        inp_model_name = f"model.{simbol}.H1.120.{model_suffix}.onnx"
    else:
        inp_model_name = f"model.{simbol}.H1.120.onnx"
    
    print(f"\n{'='*60}")
    print(f"Procesando {simbol} - {model_suffix if model_suffix else 'Trading'}")
    print(f"Per√≠odo: {start_date.strftime('%Y-%m-%d')} hasta {end_date.strftime('%Y-%m-%d')}")
    print(f"{'='*60}\n")
    
    try:
        # Leer datos desde CSV en Drive
        csv_file = f"{DATA_PATH}{simbol}_H1.csv"
        
        if not os.path.exists(csv_file):
            print(f"‚ùå ERROR: No se encontr√≥ el archivo {csv_file}")
            print(f"   Verifica que el archivo exista en Drive")
            return False
        
        # Leer CSV con separador de tabulaci√≥n
        df = pd.read_csv(csv_file, sep='\t')
        print(f"üìä Columnas encontradas: {df.columns.tolist()}")
        
        # Convertir columna DATE y TIME a datetime
        df['datetime'] = pd.to_datetime(df['<DATE>'] + ' ' + df['<TIME>'], format='%Y.%m.%d %H:%M:%S')
        
        # Filtrar por rango de fechas
        df = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)]
        
        if len(df) == 0:
            print(f"‚ùå ERROR: No hay datos en el rango de fechas para {simbol}")
            return False
        
        print(f"‚úì Datos cargados: {len(df)} registros")
        
        # Get close prices only
        data = df[['<CLOSE>']].values
        
        # Scale data
        scaler = MinMaxScaler(feature_range=(0,1))
        scaled_data = scaler.fit_transform(data)
        
        # Training size is 80% of the data
        training_size = int(len(scaled_data)*0.80) 
        print(f"‚úì Training_size: {training_size}")
        train_data_initial = scaled_data[0:training_size,:]
        test_data_initial = scaled_data[training_size:,:1]
        
        # Split into samples
        time_step = inp_history_size
        x_train, y_train = split_sequence(train_data_initial, time_step)
        x_test, y_test = split_sequence(test_data_initial, time_step)
        
        # Reshape input to be [samples, time steps, features] which is required for LSTM
        x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
        x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)
        
        print(f"‚úì x_train shape: {x_train.shape}")
        print(f"‚úì x_test shape: {x_test.shape}")
        
        # Define model
        model = Sequential()
        model.add(Conv1D(filters=256, kernel_size=2, activation='relu', padding='same', input_shape=(inp_history_size,1)))
        model.add(MaxPooling1D(pool_size=2))
        model.add(LSTM(100, return_sequences=True))
        model.add(Dropout(0.3))
        model.add(LSTM(100, return_sequences=False))
        model.add(Dropout(0.3))
        model.add(Dense(units=1, activation='sigmoid'))
        model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])
        
        print("‚úì Modelo definido")
        
        # Configurar Early Stopping
        early_stop = EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        )
        
        # Model training con early stopping
        print(f"\nüöÄ Entrenando modelo para {simbol} ({model_suffix if model_suffix else 'Trading'})...")
        history = model.fit(
            x_train, y_train, 
            epochs=3, 
            validation_data=(x_test, y_test), 
            batch_size=32, 
            callbacks=[early_stop],
            verbose=1
        )
        
        # Evaluate training data
        train_loss, train_rmse = model.evaluate(x_train, y_train, batch_size=32, verbose=0)
        print(f"\nüìà train_loss={train_loss:.3f}")
        print(f"üìà train_rmse={train_rmse:.3f}")
        
        # Evaluate testing data
        test_loss, test_rmse = model.evaluate(x_test, y_test, batch_size=32, verbose=0)
        print(f"üìâ test_loss={test_loss:.3f}")
        print(f"üìâ test_rmse={test_rmse:.3f}")
        
        # Export as TensorFlow SavedModel
        temp_model_path = f"/tmp/temp_model_{simbol}_{model_suffix if model_suffix else 'trading'}"
        
        # Remove temp directory if exists
        if os.path.exists(temp_model_path):
            shutil.rmtree(temp_model_path)
        
        # Export model (usar export en lugar de save para TensorFlow SavedModel)
        print(f"\nüíæ Exportando modelo temporal...")
        model.export(temp_model_path)
        
        # Convert to ONNX using command line tool
        output_path = MODEL_PATH + inp_model_name
        print(f"üîÑ Convirtiendo a ONNX...")
        cmd = f'python -m tf2onnx.convert --saved-model "{temp_model_path}" --output "{output_path}" --opset 13'
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"‚úÖ Modelo guardado en: {output_path}")
        else:
            print(f"‚ùå Error guardando modelo:")
            print(result.stderr)
            return False
        
        # Clean up temp directory
        if os.path.exists(temp_model_path):
            shutil.rmtree(temp_model_path)
        
        print(f"\n‚úÖ {simbol} - {model_suffix if model_suffix else 'Trading'} completado exitosamente!\n")
        return True
        
    except Exception as e:
        print(f"\n‚ùå ERROR procesando {simbol} - {model_suffix if model_suffix else 'Trading'}: {str(e)}\n")
        import traceback
        traceback.print_exc()
        return False

# ===================================================================
# PROCESO PRINCIPAL
# ===================================================================

print("\n" + "="*60)
print("üöÄ INICIANDO ENTRENAMIENTO DE MODELOS")
print("="*60 + "\n")

# Procesar cada s√≠mbolo con dos modelos
for simbol in symbols:
    print(f"\n{'#'*60}")
    print(f"# INICIANDO PROCESAMIENTO DE {simbol}")
    print(f"{'#'*60}\n")
    
    # Modelo 1: Trading (fecha actual)
    end_date_trading = datetime.now()
    success_trading = train_and_save_model(simbol, end_date_trading, model_suffix="")
    
    # Modelo 2: Backtesting (hace 7 d√≠as)
    end_date_backtesting = datetime.now() - timedelta(days=7)
    success_backtesting = train_and_save_model(simbol, end_date_backtesting, model_suffix="backtesting")
    
    if success_trading and success_backtesting:
        print(f"\n{'='*60}")
        print(f"‚úÖ‚úÖ AMBOS MODELOS PARA {simbol} COMPLETADOS EXITOSAMENTE")
        print(f"{'='*60}\n")
    else:
        print(f"\n{'='*60}")
        print(f"‚ö†Ô∏è ALGUNOS MODELOS PARA {simbol} FALLARON")
        print(f"{'='*60}\n")

print(f"\n{'#'*60}")
print("# ‚úÖ PROCESO COMPLETADO PARA TODOS LOS S√çMBOLOS")
print(f"{'#'*60}")

print(f"\nüìÅ Modelos guardados en: {MODEL_PATH}")
print("\nüéâ ¬°Entrenamiento finalizado!")
