!pip list | grep tensorflow
!which python



# Montar Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Desinstalar versiones conflictivas
!pip uninstall -y tensorflow protobuf tf2onnx

# Instalar todo compatible
!pip install tensorflow==2.19.0 protobuf==3.20.3 tf-keras tf2onnx

print("\nâœ… InstalaciÃ³n completa!")
print("âš ï¸ IMPORTANTE: Ve a Runtime > Restart runtime")








print("âœ“ LibrerÃ­as instaladas correctamente\n")
print("âœ“ LibrerÃ­as importadas correctamente")

# ===================================================================
# IMPORTAR LIBRERÃAS
# ===================================================================
import tensorflow as tf
print(tf.__version__)

import numpy as np
import pandas as pd
import os
from datetime import timedelta, datetime
from sklearn.preprocessing import MinMaxScaler
import tf2onnx

# Importar Keras (versiÃ³n standalone compatible)
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Dropout, LSTM
from keras.metrics import RootMeanSquaredError

print("âœ“ LibrerÃ­as importadas correctamente")
print(f"TensorFlow version: {tf.__version__}")
print(f"tf2onnx version: {tf2onnx.__version__}\n")

# ===================================================================
# CONFIGURAR RUTAS
# ===================================================================
DATA_PATH = '/content/drive/MyDrive/forex_data/'
MODEL_PATH = '/content/drive/MyDrive/forex_models/'

os.makedirs(MODEL_PATH, exist_ok=True)

print(f"ğŸ“‚ Data path: {DATA_PATH}")
print(f"ğŸ’¾ Model path: {MODEL_PATH}\n")

# ===================================================================
# PARÃMETROS
# ===================================================================
symbols = [ "EURUSD", "GBPUSD", "USDCHF", "AUDUSD"]
inp_history_size = 120
DAYS_TO_USE = 120

# ===================================================================
# FUNCIONES
# ===================================================================

def split_sequence(sequence, n_steps):
    """Split a univariate sequence into samples"""
    X, y = list(), list()
    for i in range(len(sequence)):
       end_ix = i + n_steps
       if end_ix > len(sequence)-1:
          break
       seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
       X.append(seq_x)
       y.append(seq_y)
    return np.array(X), np.array(y)

def train_and_save_model(simbol, end_date, model_suffix=""):
    """
    Trains a model for a given symbol and time period.
    """
    start_date = end_date - timedelta(days=DAYS_TO_USE)

    if model_suffix:
        inp_model_name = f"model.{simbol}.H1.120.{model_suffix}.onnx"
    else:
        inp_model_name = f"model.{simbol}.H1.120.onnx"

    print(f"\n{'='*60}")
    print(f"Procesando {simbol} - {model_suffix if model_suffix else 'Trading'}")
    print(f"PerÃ­odo: {start_date.strftime('%Y-%m-%d')} hasta {end_date.strftime('%Y-%m-%d')}")
    print(f"{'='*60}\n")

    try:
        # Leer datos desde CSV
        csv_file = f"{DATA_PATH}{simbol}_H1.csv"

        if not os.path.exists(csv_file):
            print(f"âŒ ERROR: No se encontrÃ³ {csv_file}")
            return False

        df = pd.read_csv(csv_file, sep='\t')
        print(f"ğŸ“Š Total registros en CSV: {len(df)}")

        # Convertir a datetime
        df['datetime'] = pd.to_datetime(df['<DATE>'] + ' ' + df['<TIME>'], format='%Y.%m.%d %H:%M:%S')

        # Filtrar Ãºltimos 120 dÃ­as
        df = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)]

        if len(df) == 0:
            print(f"âŒ ERROR: No hay datos en el rango de fechas")
            return False

        print(f"âœ“ Datos filtrados: {len(df)} registros")
        print(f"  Rango: {df['datetime'].min()} a {df['datetime'].max()}")

        # Get close prices
        data = df[['<CLOSE>']].values

        # Scale data
        scaler = MinMaxScaler(feature_range=(0,1))
        scaled_data = scaler.fit_transform(data)

        # Training 80%
        training_size = int(len(scaled_data)*0.80)
        print(f"âœ“ Training_size: {training_size}")
        train_data_initial = scaled_data[0:training_size,:]
        test_data_initial = scaled_data[training_size:,:1]

        # Split into samples
        time_step = inp_history_size
        x_train, y_train = split_sequence(train_data_initial, time_step)
        x_test, y_test = split_sequence(test_data_initial, time_step)

        # Reshape for LSTM
        x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
        x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

        print(f"âœ“ x_train shape: {x_train.shape}")
        print(f"âœ“ x_test shape: {x_test.shape}")

        # Define model - IGUAL QUE EL SCRIPT ORIGINAL
        model = Sequential()
        model.add(Conv1D(filters=256, kernel_size=2, activation='relu', padding='same', input_shape=(inp_history_size,1)))
        model.add(MaxPooling1D(pool_size=2))
        model.add(LSTM(100, return_sequences=True))
        model.add(Dropout(0.3))
        model.add(LSTM(100, return_sequences=False))
        model.add(Dropout(0.3))
        model.add(Dense(units=1, activation='sigmoid'))
        model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])

        print("âœ“ Modelo definido")

        # Model training
        print(f"\nğŸš€ Entrenando modelo...")
        history = model.fit(
            x_train, y_train,
            epochs=3,
            validation_data=(x_test, y_test),
            batch_size=32,
            verbose=2
        )

        # Evaluate
        train_loss, train_rmse = model.evaluate(x_train, y_train, batch_size=32)
        print(f"\nğŸ“ˆ train_loss={train_loss:.3f}")
        print(f"ğŸ“ˆ train_rmse={train_rmse:.3f}")

        test_loss, test_rmse = model.evaluate(x_test, y_test, batch_size=32)
        print(f"ğŸ“‰ test_loss={test_loss:.3f}")
        print(f"ğŸ“‰ test_rmse={test_rmse:.3f}")

        # Save to ONNX - IGUAL QUE EL SCRIPT ORIGINAL
        output_path = MODEL_PATH + inp_model_name
        print(f"\nğŸ’¾ Guardando modelo ONNX en: {output_path}")
        
        onnx_model = tf2onnx.convert.from_keras(model, output_path=output_path)
        
        print(f"âœ… Modelo guardado exitosamente\n")
        return True

    except Exception as e:
        print(f"\nâŒ ERROR: {str(e)}\n")
        import traceback
        traceback.print_exc()
        return False

# ===================================================================
# PROCESO PRINCIPAL
# ===================================================================

print("\n" + "="*60)
print("ğŸš€ INICIANDO ENTRENAMIENTO DE MODELOS")
print(f"ğŸ“… Ãšltimos {DAYS_TO_USE} dÃ­as de datos")
print("="*60 + "\n")

for simbol in symbols:
    print(f"\n{'#'*60}")
    print(f"# {simbol}")
    print(f"{'#'*60}\n")

    # Modelo Trading (Ãºltimos 120 dÃ­as desde hoy)
    end_date_trading = datetime.now()
    success_trading = train_and_save_model(simbol, end_date_trading, model_suffix="")

    # Modelo Backtesting (Ãºltimos 120 dÃ­as desde hace 7 dÃ­as)
    end_date_backtesting = datetime.now() - timedelta(days=7)
    success_backtesting = train_and_save_model(simbol, end_date_backtesting, model_suffix="backtesting")

    if success_trading and success_backtesting:
        print(f"\nâœ…âœ… {simbol} - AMBOS MODELOS COMPLETADOS")
    else:
        print(f"\nâš ï¸ {simbol} - ALGUNOS MODELOS FALLARON")

print(f"\n{'#'*60}")
print("# âœ… PROCESO COMPLETADO")
print(f"{'#'*60}")
print(f"\nğŸ“ Modelos en: {MODEL_PATH}")
print("\nğŸ‰ Â¡Finalizado!")



